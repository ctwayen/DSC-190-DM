{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from dateutil.rrule import rrule, DAILY \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import r2_score \n",
    "import math \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import FunctionTransformer \n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import re\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import nltk\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    # This function could remove all stop words like the, a, I\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(data) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in symbols:\n",
    "        data = str.replace(data, i, ' ')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return str.replace(data, \"'\", \"\")\n",
    "\n",
    "def remove_single_characters(data):\n",
    "    new_text = \"\"\n",
    "    words = word_tokenize(data)\n",
    "    for w in words:\n",
    "        if len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def stemming(data):\n",
    "    return PorterStemmer().stem(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = str(data)\n",
    "    data = data.lower()\n",
    "    data = remove_punctuation(data)\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_single_characters(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xinrui zhan\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# read section\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra people\n",
    "train_df.extra_people = train_df.extra_people.apply(lambda x: int(x[:-3].replace('$', '')))\n",
    "test_df.extra_people = test_df.extra_people.apply(lambda x: int(x[:-3].replace('$', '')))\n",
    "def trans_ep(ele):\n",
    "    if ele == 0:\n",
    "        return 0\n",
    "    elif ele < 10:\n",
    "        return 1\n",
    "    elif ele < 20:\n",
    "        return 2\n",
    "    elif ele < 30:\n",
    "        return 3\n",
    "    elif ele < 50:\n",
    "        return 4\n",
    "    elif ele < 80:\n",
    "        return 5\n",
    "    elif ele < 150:\n",
    "        return 6\n",
    "    else:\n",
    "        return 7\n",
    "train_df.extra_people = train_df.extra_people.apply(trans_ep)\n",
    "test_df.extra_people = test_df.extra_people.apply(trans_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary , words are picked from tfidf\n",
    "train_df.summary = train_df.summary.apply(preprocess)\n",
    "test_df.summary = test_df.summary.apply(preprocess)\n",
    "summary_lst_high = ['sonder', 'luxurious', '45th', 'kettl', 'typically', 'LUX', 'superbowl', 'floorplan', 'wyndham']\n",
    "summary_lst_low = ['cat', 'shared', 'roommate', 'airline', 'bedly', 'turnkey']\n",
    "def extract_sum_1(summary):\n",
    "    words = word_tokenize(summary)\n",
    "    res1 = 0\n",
    "    for i in summary_lst_high:\n",
    "        if i in words:\n",
    "            res1 += 1\n",
    "    return res1\n",
    "def extract_sum_2(summary):\n",
    "    words = word_tokenize(summary)\n",
    "    res2 = 0\n",
    "    for i in summary_lst_low:\n",
    "        if i in words:\n",
    "            res2 += 1\n",
    "    return res2\n",
    "\n",
    "train_df['summary_1'] = train_df.summary.apply(extract_sum_1)\n",
    "test_df['summary_1'] = test_df.summary.apply(extract_sum_1)\n",
    "train_df['summary_2'] = train_df.summary.apply(extract_sum_2)\n",
    "test_df['summary_2'] = test_df.summary.apply(extract_sum_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "# description, words are picked from tfidf\n",
    "train_df.description = train_df.description.apply(preprocess)\n",
    "test_df.description = test_df.description.apply(preprocess)\n",
    "des_lst_high = ['superbowl', 'cinemax', 'hard', 'nearest', 'matter']\n",
    "des_lst_low = ['laundromat', 'cat', 'laguardia', 'female', 'fan', ]\n",
    "def extract_des_1(summary):\n",
    "    words = word_tokenize(summary)\n",
    "    res1 = 0\n",
    "    for i in summary_lst_high:\n",
    "        if i in words:\n",
    "            res1 += 1\n",
    "    return res1\n",
    "\n",
    "def extract_des_2(summary):\n",
    "    words = word_tokenize(summary)\n",
    "    res2 = 0\n",
    "    for i in summary_lst_low:\n",
    "        if i in words:\n",
    "            res2 += 1\n",
    "    return res2\n",
    "\n",
    "train_df['des_1'] = train_df.summary.apply(extract_des_1)\n",
    "test_df['des_1'] = test_df.summary.apply(extract_des_1)\n",
    "train_df['des_2'] = train_df.summary.apply(extract_des_2)\n",
    "test_df['des_2'] = test_df.summary.apply(extract_des_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transit, words are picked from tfidf\n",
    "train_df.transit = train_df.transit.apply(preprocess)\n",
    "test_df.transit = test_df.transit.apply(preprocess)trans_lst_high = ['wyndham', '•34', 'rockette', 'highline', 'sampling', 'headquarters', 'kicks', '•nyc',   ]\n",
    "trans_lst_low = ['along', 'utica', 'road', 'transfer', 'myrtle', 'av', 'parkway', 'quick', ]\n",
    "def extract_trans_1(summary):\n",
    "    words = word_tokenize(summary)\n",
    "    res1 = 0\n",
    "    for i in trans_lst_high:\n",
    "        if i in words:\n",
    "            res1 += 1\n",
    "    return res1\n",
    "\n",
    "def extract_trans_2(summary):\n",
    "    words = word_tokenize(summary)\n",
    "    res2 = 0\n",
    "    for i in trans_lst_low:\n",
    "        if i in words:\n",
    "            res2 += 1\n",
    "    return res2\n",
    "\n",
    "train_df['trans_1'] = train_df.summary.apply(extract_trans_1)\n",
    "test_df['trans_1'] = test_df.summary.apply(extract_trans_1)\n",
    "train_df['trans_2'] = train_df.summary.apply(extract_trans_2)\n",
    "test_df['trans_2'] = test_df.summary.apply(extract_trans_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipcode, only using the first five\n",
    "train_df.zipcode = train_df.zipcode.apply(lambda x: str(x)[:5])\n",
    "test_df.zipcode = test_df.zipcode.apply(lambda x: str(x)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self one hot the amenities, \n",
    "train_df.amenities = train_df.amenities.apply(preprocess).apply(lambda x: word_tokenize(x))\n",
    "test_df.amenities = test_df.amenities.apply(preprocess).apply(lambda x: word_tokenize(x))\n",
    "amenities_dct = {}\n",
    "for lst in train_df.amenities:\n",
    "    for item in lst:\n",
    "        if item not in amenities_dct.keys():\n",
    "            amenities_dct[item] = 1\n",
    "        else:\n",
    "            amenities_dct[item]+=1\n",
    "values = np.array(list(amenities_dct.values()))\n",
    "keys = np.array(list(amenities_dct.keys()))\n",
    "ind = (values > 500) & (values < 34000)\n",
    "display(len(keys[ind]))\n",
    "related = keys[ind]\n",
    "\n",
    "def one_hot(lst):\n",
    "    temp = np.zeros(157)\n",
    "    for i in range(len(related)):\n",
    "        if related[i] in lst:\n",
    "            temp[i] = 1\n",
    "    return temp\n",
    "\n",
    "amenities_oh = train_df.amenities.apply(one_hot)\n",
    "train_df['amenities_oh'] = amenities_oh\n",
    "test_df['amenities_oh'] = test_df.amenities.apply(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether a real bed\n",
    "def whether_realbed(bed):\n",
    "    if bed == 'Real Bed':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "train_df.bed_type = train_df.bed_type.apply(whether_realbed)\n",
    "test_df.bed_type = test_df.bed_type.apply(whether_realbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, split\n",
    "train_X, val_X, y_train, y_val = train_test_split(train_df.drop(columns=['price']), train_df.price, test_size = 0.5)\n",
    "train_X.loc[:, 'Y'] = y_train\n",
    "val_X.loc[:, 'Y'] = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is reserved for later. Since sklearn ordinal coding does not have handle unknown option, we need to manually\n",
    "# change the unseen value to most frequent before we run.\n",
    "val_X.zipcode = val_X.zipcode.apply(lambda x: '11211' if x in ['10119', '91766', '10162', '11621', '11461', '10281', '10106', '11363']  else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare transform function for late pipeline\n",
    "property_list = ['Apartment', 'House', 'Townhouse', 'Loft', 'Condominium', 'Serviced apartment', 'Guest suite', 'none']\n",
    "def prop_help(ele):\n",
    "    if ele in property_list:\n",
    "        return ele\n",
    "    else:\n",
    "        return 'else'\n",
    "    \n",
    "def tran_proper(col):\n",
    "    col = col.fillna('none')\n",
    "    col.property_type = col.property_type.apply(prop_help)\n",
    "    return col\n",
    "\n",
    "nei_lst = list(train_df.neighbourhood_cleansed.value_counts().index[:10])\n",
    "def transform_help(ele):\n",
    "    if ele in nei_lst:\n",
    "        return ele\n",
    "    else:\n",
    "        return 'else'\n",
    "    \n",
    "def transform_nei(col):\n",
    "    col = col.fillna('none')\n",
    "    col.neighbourhood_cleansed = col.neighbourhood_cleansed.apply(transform_help)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing pipeline\n",
    "num_feat = ['accommodates', 'beds', 'bathrooms', 'bedrooms', 'reviews_per_month', 'guests_included', 'minimum_nights', 'maximum_nights']\n",
    "num_t = Pipeline(steps=[\n",
    "    ('imp', SimpleImputer(missing_values=np.nan, strategy='mean'))\n",
    "])\n",
    "\n",
    "str_feat = ['room_type', 'review_scores_location', 'review_scores_cleanliness']\n",
    "str_t = Pipeline(steps=[\n",
    "    ('imp', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "nei_feat_gr = ['neighbourhood_group_cleansed']\n",
    "nei_t_gr = Pipeline(steps=[\n",
    "    ('imp', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "    ('one-hot', OneHotEncoder(handle_unknown = 'ignore', sparse=False))\n",
    "])\n",
    "\n",
    "prop_feat = ['property_type']\n",
    "prop_t = Pipeline(steps=[\n",
    "    ('property', FunctionTransformer(tran_proper, validate=False)),\n",
    "    ('ordinal', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "sum_feat = ['summary_1', 'summary_2']\n",
    "sum_t = Pipeline(steps=[\n",
    "    ('property', FunctionTransformer(lambda x: x, validate=False))\n",
    "])\n",
    "\n",
    "des_feat = ['des_1', 'des_2']\n",
    "des_t = Pipeline(steps=[\n",
    "    ('property', FunctionTransformer(lambda x: x, validate=False))\n",
    "])\n",
    "\n",
    "nei_feat = ['neighbourhood_cleansed']\n",
    "nei_t = Pipeline(steps=[\n",
    "    ('nei', FunctionTransformer(transform_nei, validate=False)),\n",
    "    ('ordinal', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "zip_feat = ['zipcode']\n",
    "zip_t = Pipeline(steps=[\n",
    "    ('imp', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "extra_feat = ['extra_people']\n",
    "extra_t = Pipeline(steps=[\n",
    "    ('imp', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    ('property', FunctionTransformer(lambda x: x, validate=False))\n",
    "    #('ordinal', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "amenities_feat = ['amenities_oh']\n",
    "amenities_t = Pipeline(steps=[\n",
    "    ('property', FunctionTransformer(lambda x: x, validate=False))\n",
    "])\n",
    "\n",
    "bed_feat = ['bed_type']\n",
    "bed_t = Pipeline(steps=[\n",
    "    ('property', FunctionTransformer(lambda x: x, validate=False))\n",
    "])\n",
    "\n",
    "trans_feat = ['trans_1', 'trans_2']\n",
    "trans_t = Pipeline(steps=[\n",
    "    ('property', FunctionTransformer(lambda x: x, validate=False))\n",
    "])\n",
    "\n",
    "preproc = ColumnTransformer(transformers=[\n",
    "    ('num', num_t, num_feat),\n",
    "    ('str', str_t, str_feat),\n",
    "    ('nei_gr', nei_t_gr, nei_feat_gr),\n",
    "    ('prop', prop_t, prop_feat),\n",
    "    ('sum', sum_t, sum_feat),\n",
    "    ('nei', nei_t, nei_feat),\n",
    "    ('des', des_t, des_feat),\n",
    "    ('zip', zip_t, zip_feat),\n",
    "    ('extra', extra_t, extra_feat),\n",
    "    ('bed', bed_t, bed_feat),\n",
    "    ('trans', trans_t, trans_feat),\n",
    "    ('amenities', amenities_t, amenities_feat)\n",
    "])\n",
    "\n",
    "plf = Pipeline(steps=[('prep', preproc)]) #, ('regr', rf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('prep',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('imp',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value=None,\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='mean',\n",
       "                                                                                 verbose=0))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['accommodates', 'beds',\n",
       "                                                   'bathrooms', 'bedroo...\n",
       "                                                                                       validate=False))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['trans_1', 'trans_2']),\n",
       "                                                 ('amenities',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('property',\n",
       "                                                                   FunctionTransformer(accept_sparse=False,\n",
       "                                                                                       check_inverse=True,\n",
       "                                                                                       func=<function <lambda> at 0x000001A2EA60B1E0>,\n",
       "                                                                                       inv_kw_args=None,\n",
       "                                                                                       inverse_func=None,\n",
       "                                                                                       kw_args=None,\n",
       "                                                                                       pass_y='deprecated',\n",
       "                                                                                       validate=False))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['amenities_oh'])],\n",
       "                                   verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit whole data, this is called when I wanna submit\n",
    "plf.fit(train_df.drop(columns=['price']), train_df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('prep',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('imp',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value=None,\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='mean',\n",
       "                                                                                 verbose=0))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['accommodates', 'beds',\n",
       "                                                   'bathrooms', 'bedroo...\n",
       "                                                                                       validate=False))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['trans_1', 'trans_2']),\n",
       "                                                 ('amenities',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('property',\n",
       "                                                                   FunctionTransformer(accept_sparse=False,\n",
       "                                                                                       check_inverse=True,\n",
       "                                                                                       func=<function <lambda> at 0x000001A2EA60B1E0>,\n",
       "                                                                                       inv_kw_args=None,\n",
       "                                                                                       inverse_func=None,\n",
       "                                                                                       kw_args=None,\n",
       "                                                                                       pass_y='deprecated',\n",
       "                                                                                       validate=False))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['amenities_oh'])],\n",
       "                                   verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the train data, called when I am modeling\n",
    "plf.fit(train_X.drop(columns=['Y']), train_X['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same with the reserved section, this is used for handling the ordinal encoding\n",
    "test_df.zipcode = test_df.zipcode.apply(lambda x: '11211' if x in ['10704', '11954', '10550', '10174', '1m', '11239', '10107', '11121'] else x)\n",
    "#plf.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will flatt the self coded onehot econding return array\n",
    "def flatt(a):\n",
    "    new = []\n",
    "    for sa in a:\n",
    "        new.append(np.concatenate((sa[:-1], sa[-1]), axis=None))\n",
    "    return np.array(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xinrui zhan\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tvalid_0's rmse: 97.6807\n",
      "[40]\tvalid_0's rmse: 86.9255\n",
      "[60]\tvalid_0's rmse: 82.7402\n",
      "[80]\tvalid_0's rmse: 80.0205\n",
      "[100]\tvalid_0's rmse: 78.1961\n",
      "[120]\tvalid_0's rmse: 76.5421\n",
      "[140]\tvalid_0's rmse: 74.9907\n",
      "[160]\tvalid_0's rmse: 74.3162\n",
      "[180]\tvalid_0's rmse: 73.4645\n",
      "[200]\tvalid_0's rmse: 71.9521\n",
      "[220]\tvalid_0's rmse: 70.5647\n",
      "[240]\tvalid_0's rmse: 69.6749\n",
      "[260]\tvalid_0's rmse: 68.9227\n",
      "[280]\tvalid_0's rmse: 67.9565\n",
      "[300]\tvalid_0's rmse: 67.1433\n",
      "[320]\tvalid_0's rmse: 66.3346\n",
      "[340]\tvalid_0's rmse: 65.7866\n",
      "[360]\tvalid_0's rmse: 65.1429\n",
      "[380]\tvalid_0's rmse: 64.4683\n",
      "[400]\tvalid_0's rmse: 63.9526\n",
      "[420]\tvalid_0's rmse: 63.6448\n",
      "[440]\tvalid_0's rmse: 63.0612\n",
      "[460]\tvalid_0's rmse: 62.7018\n",
      "[480]\tvalid_0's rmse: 62.2286\n",
      "[500]\tvalid_0's rmse: 61.7055\n",
      "[520]\tvalid_0's rmse: 61.2082\n",
      "[540]\tvalid_0's rmse: 60.7847\n",
      "[560]\tvalid_0's rmse: 60.4525\n",
      "[580]\tvalid_0's rmse: 60.1931\n",
      "[600]\tvalid_0's rmse: 59.7693\n",
      "[620]\tvalid_0's rmse: 59.0865\n",
      "[640]\tvalid_0's rmse: 58.558\n",
      "[660]\tvalid_0's rmse: 58.1447\n",
      "[680]\tvalid_0's rmse: 57.8015\n",
      "[700]\tvalid_0's rmse: 57.5114\n",
      "[720]\tvalid_0's rmse: 57.1331\n",
      "[740]\tvalid_0's rmse: 56.6999\n",
      "[760]\tvalid_0's rmse: 56.4507\n",
      "[780]\tvalid_0's rmse: 56.0892\n",
      "[800]\tvalid_0's rmse: 55.7521\n",
      "[820]\tvalid_0's rmse: 55.3478\n",
      "[840]\tvalid_0's rmse: 54.9831\n",
      "[860]\tvalid_0's rmse: 54.7155\n",
      "[880]\tvalid_0's rmse: 54.5015\n",
      "[900]\tvalid_0's rmse: 54.0176\n",
      "[920]\tvalid_0's rmse: 53.7778\n",
      "[940]\tvalid_0's rmse: 53.5495\n",
      "[960]\tvalid_0's rmse: 53.1932\n",
      "[980]\tvalid_0's rmse: 52.8059\n",
      "[1000]\tvalid_0's rmse: 52.6765\n",
      "[1020]\tvalid_0's rmse: 52.3807\n",
      "[1040]\tvalid_0's rmse: 52.096\n",
      "[1060]\tvalid_0's rmse: 51.8636\n",
      "[1080]\tvalid_0's rmse: 51.5499\n",
      "[1100]\tvalid_0's rmse: 51.3234\n",
      "[1120]\tvalid_0's rmse: 51.0681\n",
      "[1140]\tvalid_0's rmse: 50.8925\n",
      "[1160]\tvalid_0's rmse: 50.6518\n",
      "[1180]\tvalid_0's rmse: 50.4484\n",
      "[1200]\tvalid_0's rmse: 50.2366\n",
      "[1220]\tvalid_0's rmse: 49.8652\n",
      "[1240]\tvalid_0's rmse: 49.6462\n",
      "[1260]\tvalid_0's rmse: 49.4139\n",
      "[1280]\tvalid_0's rmse: 49.1769\n",
      "[1300]\tvalid_0's rmse: 48.9658\n",
      "[1320]\tvalid_0's rmse: 48.8531\n",
      "[1340]\tvalid_0's rmse: 48.7263\n",
      "[1360]\tvalid_0's rmse: 48.5105\n",
      "[1380]\tvalid_0's rmse: 48.3967\n",
      "[1400]\tvalid_0's rmse: 48.1262\n",
      "[1420]\tvalid_0's rmse: 47.9505\n",
      "[1440]\tvalid_0's rmse: 47.7121\n",
      "[1460]\tvalid_0's rmse: 47.492\n",
      "[1480]\tvalid_0's rmse: 47.2344\n",
      "[1500]\tvalid_0's rmse: 46.947\n"
     ]
    }
   ],
   "source": [
    "# light gbm model parameters\n",
    "params = {\n",
    "        'boosting_type':'dart',\n",
    "        'objective': 'regression',\n",
    "        'num_leaves': 42,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': -1,\n",
    "        'subsample': 0.8,\n",
    "        'bagging_fraction' : 1,\n",
    "        'bagging_freq': 20,\n",
    "        'metric': 'rmse',\n",
    "        'min_split_gain': 0.5,\n",
    "        'min_child_weight': 1,\n",
    "        'min_data_in_leaf': 5,\n",
    "        'scale_pos_weight':1,\n",
    "        'num_rounds':1500,\n",
    "        'lambda_l2': 0.05,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'xgboost_dart_mode': True,\n",
    "    }\n",
    "# below two will called when I wanna submit\n",
    "train_set = lgbm.Dataset(flatt(plf.transform(train_df)), train_df['price'])\n",
    "valid_set = lgbm.Dataset(flatt(plf.transform(train_df)), train_df['price'])\n",
    "\n",
    "# below two will called when modeling\n",
    "#train_set = lgbm.Dataset(flatt(plf.transform(train_X)), train_X['Y'])\n",
    "#valid_set = lgbm.Dataset(flatt(plf.transform(val_X)), val_X[\"Y\"])\n",
    "\n",
    "model = lgbm.train(params, train_set = train_set, \n",
    "                   verbose_eval=20, valid_sets=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ordinal encoding\n",
    "test_df.loc[test_df.review_scores_location == 3, 'review_scores_location'] = 2\n",
    "\n",
    "prediction = model.predict(flatt(plf.transform(test_df)), num_iteration = model.best_iteration)\n",
    "test_df['Predicted'] =  pd.Series(prediction)\n",
    "\n",
    "test_df.loc[:, ['id', 'Predicted']].to_csv('boosting.csv', index=False)\n",
    "boot = pd.read_csv('boosting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2521    31.796377\n",
       "Name: Predicted, dtype: float64"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boot[boot.id == 25423395]['Predicted']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
